{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REFDNN:\n",
    "    def __init__(self,\n",
    "                 hidden_units,\n",
    "                 learning_rate_ftrl,\n",
    "                 learning_rate_adam,\n",
    "                 l1_regularization_strength,\n",
    "                 l2_regularization_strength,\n",
    "                 batch_size=32,\n",
    "                 training_steps=10000,\n",
    "                 evaluation_steps=100,\n",
    "                 earlystop_use=True, patience=20,\n",
    "                 checkpoint_path=None):\n",
    "        \n",
    "        ## hyperparameters\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate_ftrl = learning_rate_ftrl\n",
    "        self.learning_rate_adam = learning_rate_adam\n",
    "        self.l1_regularization_strength = l1_regularization_strength\n",
    "        self.l2_regularization_strength = l2_regularization_strength\n",
    "        \n",
    "        ## parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.training_steps = training_steps\n",
    "        self.evaluation_steps = evaluation_steps\n",
    "        self.earlystop_use = earlystop_use\n",
    "        self.patience = patience\n",
    "        \n",
    "        ## environment\n",
    "        if checkpoint_path is None:\n",
    "            checkpoint_path = \"ckpt_RefDNN.ckpt\"\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            X_train, S_train, I_train, Y_train,\n",
    "            X_valid, S_valid, I_valid, Y_valid,\n",
    "            verbose=2):\n",
    "\n",
    "        ## constant\n",
    "        epsilon = 1e-5\n",
    "        threshold = 0.5\n",
    "\n",
    "        ## data information\n",
    "        self._X_shape = (None, X_train.shape[1]) # number of genes\n",
    "        self._S_shape = (None, S_train.shape[1]) # number of drugs\n",
    "\n",
    "        ## tf.Session\n",
    "        self._open_session()\n",
    "\n",
    "        ## tf.placeholder\n",
    "        X_train_PH = tf.compat.v1.placeholder(shape=self._X_shape, dtype=tf.float32, name='X_train_PH')\n",
    "        S_train_PH = tf.compat.v1.placeholder(shape=self._S_shape, dtype=tf.float32, name='S_train_PH')\n",
    "        I_train_PH = tf.compat.v1.placeholder(shape=(None,), dtype=tf.int32, name='I_train_PH')\n",
    "        Y_train_PH = tf.compat.v1.placeholder(shape=(None,1), dtype=tf.uint8, name='Y_train_PH')\n",
    "\n",
    "        X_valid_PH = tf.compat.v1.placeholder(shape=self._X_shape, dtype=tf.float32, name='X_valid_PH')\n",
    "        S_valid_PH = tf.compat.v1.placeholder(shape=self._S_shape, dtype=tf.float32, name='S_valid_PH')\n",
    "        I_valid_PH = tf.compat.v1.placeholder(shape=(None,), dtype=tf.int32, name='I_valid_PH')\n",
    "        Y_valid_PH = tf.compat.v1.placeholder(shape=(None,1), dtype=tf.uint8, name='Y_valid_PH')\n",
    "\n",
    "        ## tf.dataset\n",
    "        dataset_train = tf.data.Dataset.from_tensor_slices((X_train_PH, S_train_PH, I_train_PH, Y_train_PH))\n",
    "        dataset_train = dataset_train.repeat()\n",
    "        dataset_train = dataset_train.shuffle(buffer_size=10000, seed=2019)\n",
    "        dataset_train = dataset_train.batch(self.batch_size, drop_remainder=False)\n",
    "        dataset_train.prefetch(2 * self.batch_size)\n",
    "\n",
    "        dataset_valid = tf.data.Dataset.from_tensor_slices((X_valid_PH, S_valid_PH, I_valid_PH, Y_valid_PH))\n",
    "        dataset_valid = dataset_valid.repeat()\n",
    "        dataset_valid = dataset_valid.batch(self.batch_size, drop_remainder=False)\n",
    "        dataset_valid.prefetch(2 * self.batch_size)\n",
    "\n",
    "        ## tf.iterator\n",
    "        iterator_train = tf.compat.v1.data.make_initializable_iterator(dataset_train)\n",
    "        iterator_valid = tf.compat.v1.data.make_initializable_iterator(dataset_valid)\n",
    "        _ = self.sess.run(iterator_train.initializer,\n",
    "                          feed_dict={X_train_PH:X_train,\n",
    "                                     S_train_PH:S_train,\n",
    "                                     I_train_PH:I_train,\n",
    "                                     Y_train_PH:Y_train})\n",
    "        _ = self.sess.run(iterator_valid.initializer,\n",
    "                          feed_dict={X_valid_PH:X_valid,\n",
    "                                     S_valid_PH:S_valid,\n",
    "                                     I_valid_PH:I_valid,\n",
    "                                     Y_valid_PH:Y_valid})\n",
    "\n",
    "        ## tf.iterator.handle\n",
    "        handle_train = self.sess.run(iterator_train.string_handle())\n",
    "        handle_valid = self.sess.run(iterator_valid.string_handle())\n",
    "\n",
    "        ## tf.graph\n",
    "        self._output_types = dataset_train.output_types\n",
    "        self._output_shapes = dataset_train.output_shapes\n",
    "\n",
    "        self._create_graph()\n",
    "\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        self.sess.run(tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "        ## history\n",
    "        history = {\n",
    "            'train':{\n",
    "                'loss':[]\n",
    "            },\n",
    "            'valid':{\n",
    "                'loss':[],\n",
    "                'acc':[],\n",
    "                'precision':[],\n",
    "                'recall':[]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ## initialization for early stopping\n",
    "        if self.earlystop_use:\n",
    "            best_loss = np.Inf\n",
    "            best_step = 0\n",
    "            cnt_patience = 0\n",
    "\n",
    "        ## initialize terminal conditions\n",
    "        termination_convergence = False\n",
    "        termination_earlystop = False\n",
    "\n",
    "        ## start fitting\n",
    "        start_time = time.time()\n",
    "        for step in range(1, self.training_steps+1):\n",
    "            \n",
    "            ## 1) training\n",
    "            _, _, loss_train = self.sess.run([self.TRAIN_ftrl, self.TRAIN_adam, self.LOSS],\n",
    "                                             feed_dict={self.handle:handle_train, self.training:True})\n",
    "\n",
    "            ## 2) evaluation\n",
    "            if step % self.evaluation_steps == 0:\n",
    "                ## 2-2) validation loss\n",
    "                loss_valid = 0.\n",
    "                acc_valid = 0.\n",
    "                precision_valid = 0.\n",
    "                recall_valid = 0.\n",
    "                n_batch_valid = math.ceil(len(X_valid) // self.batch_size)\n",
    "                for _ in range(n_batch_valid):\n",
    "                    metrics_valid = self.sess.run([self.LOSS, self.ACCURACY, self.PRECISION, self.RECALL],\n",
    "                                                  feed_dict={self.handle:handle_valid, self.training:False})\n",
    "                    loss_valid += metrics_valid[0]\n",
    "                    acc_valid += metrics_valid[1][0]\n",
    "                    precision_valid += metrics_valid[2][0]\n",
    "                    recall_valid += metrics_valid[3][0]\n",
    "                loss_valid /= n_batch_valid\n",
    "                acc_valid /= n_batch_valid\n",
    "                precision_valid /= n_batch_valid\n",
    "                recall_valid /= n_batch_valid\n",
    "\n",
    "                ## 2-3) store\n",
    "                history['train']['loss'].append((step, loss_train))\n",
    "                history['valid']['loss'].append((step, loss_valid))\n",
    "                history['valid']['acc'].append((step, acc_valid))\n",
    "                history['valid']['precision'].append((step, precision_valid))\n",
    "                history['valid']['recall'].append((step, recall_valid))\n",
    "\n",
    "                if verbose > 1:\n",
    "                    end_time = time.time()\n",
    "                    log = \"[RefDNN] [{:05d}] LOSS_train={:.5f} | LOSS_valid={:.5f}\".format(step, loss_train, loss_valid)\n",
    "                    log += \" | ACC_valid={:.3f}\".format(acc_valid)\n",
    "                    log += \" | PRECISION_valid={:.3f}\".format(precision_valid)\n",
    "                    log += \" | RECALL_valid={:.3f}\".format(recall_valid)\n",
    "                    log += \" | ({:.3f} sec)\".format(end_time-start_time)\n",
    "                    print(log)\n",
    "                    start_time = time.time()\n",
    "\n",
    "                ## 2-4) early stopping\n",
    "                if self.earlystop_use:\n",
    "                    if loss_valid < best_loss:\n",
    "                        best_loss = loss_valid\n",
    "                        best_step = step\n",
    "                        cnt_patience = 0\n",
    "                        self.saver.save(self.sess, self.checkpoint_path)\n",
    "                        if verbose > 1:\n",
    "                            print(\"[RefDNN] [CHECKPOINT] Model is saved in: {}\".format(self.checkpoint_path))\n",
    "                    elif cnt_patience < self.patience:\n",
    "                        cnt_patience += 1\n",
    "                    else:\n",
    "                        termination_earlystop = True\n",
    "\n",
    "                else:\n",
    "                    self.saver.save(self.sess, self.checkpoint_path)\n",
    "                    if verbose > 1:\n",
    "                        print(\"[RefDNN] [CHECKPOINT] Model is saved in: {}\".format(self.checkpoint_path))\n",
    "\n",
    "            ## 3) convergence\n",
    "            if len(history['train']['loss']) > 1:\n",
    "                termination_convergence = abs(history['train']['loss'][-1][1] - history['train']['loss'][-2][1]) < epsilon\n",
    "\n",
    "            ## 4) termination\n",
    "            if termination_convergence or termination_earlystop:\n",
    "                break\n",
    "\n",
    "        ## tf.Session\n",
    "        self._close_session()\n",
    "        return history\n",
    "\n",
    "\n",
    "    def predict(self, X_test, S_test, verbose=2):\n",
    "        ## dummy input data\n",
    "        I_test = np.zeros((X_test.shape[0],), dtype=np.int32)\n",
    "        Y_test = np.zeros((X_test.shape[0],1), dtype=np.uint8)\n",
    "\n",
    "        ## tf.Session\n",
    "        self._open_session()\n",
    "\n",
    "        ## tf.placeholder\n",
    "        X_test_PH = tf.compat.v1.placeholder(shape=self._X_shape, dtype=tf.float32, name='X_test_PH')\n",
    "        S_test_PH = tf.compat.v1.placeholder(shape=self._S_shape, dtype=tf.float32, name='S_test_PH')\n",
    "        I_test_PH = tf.compat.v1.placeholder(shape=(None,), dtype=tf.int32, name='I_test_PH')\n",
    "        Y_test_PH = tf.compat.v1.placeholder(shape=(None,1), dtype=tf.uint8, name='Y_test_PH')\n",
    "\n",
    "        ## tf.dataset\n",
    "        dataset_test = tf.data.Dataset.from_tensor_slices((X_test_PH, S_test_PH, I_test_PH, Y_test_PH))\n",
    "        dataset_test = dataset_test.batch(self.batch_size)\n",
    "        dataset_test.prefetch(2 * self.batch_size)\n",
    "\n",
    "        ## tf.iterator\n",
    "        iterator_test = tf.compat.v1.data.make_initializable_iterator(dataset_test)\n",
    "        _ = self.sess.run(iterator_test.initializer,\n",
    "                          feed_dict={X_test_PH:X_test,\n",
    "                                     S_test_PH:S_test,\n",
    "                                     I_test_PH:I_test,\n",
    "                                     Y_test_PH:Y_test})\n",
    "        \n",
    "        ## tf.iterator.handle\n",
    "        handle_test = self.sess.run(iterator_test.string_handle())\n",
    "\n",
    "        ## tf.graph\n",
    "        self._create_graph()\n",
    "        self.saver.restore(self.sess, self.checkpoint_path)\n",
    "        if verbose > 1:\n",
    "            print(\"[RefDNN] [CHECKPOINT] Model is restored from: {}\".format(self.checkpoint_path))\n",
    "\n",
    "        ## initialization of outputs\n",
    "        outputs = []\n",
    "\n",
    "        ## predict per batch\n",
    "        while True:\n",
    "            try:\n",
    "                predictions = self.sess.run(self.Y_main, feed_dict={self.handle:handle_test, self.training:False})\n",
    "                outputs += predictions.tolist()\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        ## tf.Session\n",
    "        self._close_session()\n",
    "        return np.array(outputs, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X_test, S_test, verbose=2):\n",
    "        ## dummy input data\n",
    "        I_test = np.zeros((X_test.shape[0],), dtype=np.int32)\n",
    "        Y_test = np.zeros((X_test.shape[0],1), dtype=np.uint8)\n",
    "\n",
    "        ## tf.Session\n",
    "        self._open_session()\n",
    "\n",
    "        ## tf.placeholder\n",
    "        X_test_PH = tf.compat.v1.placeholder(shape=self._X_shape, dtype=tf.float32, name='X_test_PH')\n",
    "        S_test_PH = tf.compat.v1.placeholder(shape=self._S_shape, dtype=tf.float32, name='S_test_PH')\n",
    "        I_test_PH = tf.compat.v1.placeholder(shape=(None,), dtype=tf.int32, name='I_test_PH')\n",
    "        Y_test_PH = tf.compat.v1.placeholder(shape=(None,1), dtype=tf.uint8, name='Y_test_PH')\n",
    "\n",
    "        ## tf.dataset\n",
    "        dataset_test = tf.data.Dataset.from_tensor_slices((X_test_PH, S_test_PH, I_test_PH, Y_test_PH))\n",
    "        dataset_test = dataset_test.batch(self.batch_size)\n",
    "        dataset_test.prefetch(2 * self.batch_size)\n",
    "\n",
    "        ## tf.iterator\n",
    "        iterator_test = tf.compat.v1.data.make_initializable_iterator(dataset_test)\n",
    "        _ = self.sess.run(iterator_test.initializer,\n",
    "                          feed_dict={X_test_PH:X_test,\n",
    "                                     S_test_PH:S_test,\n",
    "                                     I_test_PH:I_test,\n",
    "                                     Y_test_PH:Y_test})\n",
    "        ## tf.iterator.handle\n",
    "        handle_test = self.sess.run(iterator_test.string_handle())\n",
    "\n",
    "        ## tf.graph\n",
    "        self._create_graph()\n",
    "        self.saver.restore(self.sess, self.checkpoint_path)\n",
    "        if verbose > 1:\n",
    "            print(\"[RefDNN] [CHECKPOINT] Model is restored from: {}\".format(self.checkpoint_path))\n",
    "\n",
    "        ## initialization of outputs\n",
    "        outputs = []\n",
    "\n",
    "        ## predict per batch\n",
    "        while True:\n",
    "            try:\n",
    "                probabilites = self.sess.run(self.P_main, feed_dict={self.handle:handle_test, self.training:False})\n",
    "                outputs += probabilites.tolist()\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        ## tf.Session\n",
    "        self._close_session()\n",
    "        return np.array(outputs, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def get_kernels(self, hidden_names=None, verbose=2):\n",
    "        '''\n",
    "        dense0\n",
    "        dense1\n",
    "        dense2\n",
    "        output\n",
    "        '''\n",
    "        if hidden_names == None:\n",
    "            hidden_names = ['dense0', 'dense1', 'dense2', 'output']\n",
    "\n",
    "        ## tf.Session\n",
    "        self._open_session()\n",
    "        ## tf.graph\n",
    "        self._create_graph()\n",
    "        self.saver.restore(self.sess, self.checkpoint_path)\n",
    "        if verbose > 1:\n",
    "            print(\"[RefDNN] [CHECKPOINT] Model is restored from: {}\".format(self.checkpoint_path))\n",
    "        ## weights\n",
    "        kernel_dict = {}\n",
    "        for hidden_name in hidden_names:\n",
    "            weights = tf.compat.v1.get_default_graph().get_tensor_by_name('{}/kernel:0'.format(hidden_name))\n",
    "            kernel_dict[hidden_name] = self.sess.run(weights)\n",
    "        ## tf.Session\n",
    "        self._close_session()\n",
    "        return kernel_dict\n",
    "\n",
    "\n",
    "    def _create_graph(self):\n",
    "        ## tf.Placeholder\n",
    "        self.handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "        self.training = tf.compat.v1.placeholder(dtype=bool, name='training_PH')\n",
    "\n",
    "        ## tf.Iterator\n",
    "        iterator = tf.compat.v1.data.Iterator.from_string_handle(string_handle=self.handle,\n",
    "                                                       output_types=self._output_types,\n",
    "                                                       output_shapes=self._output_shapes)\n",
    "        X_batch, S_batch, I_batch, Y_batch = iterator.get_next()\n",
    "\n",
    "        ## Activation function\n",
    "        nonlinear = tf.nn.sigmoid\n",
    "\n",
    "        ## Model\n",
    "        dense0    = tf.compat.v1.layers.dense(inputs=X_batch, units=self._S_shape[1], activation='linear', name='dense0')\n",
    "        activate0 = nonlinear(dense0, name='activation0')\n",
    "\n",
    "        dense1    = tf.compat.v1.layers.dense(inputs=tf.multiply(S_batch, activate0), units=self.hidden_units, activation='linear', name='dense1')\n",
    "        bn1       = tf.compat.v1.layers.batch_normalization(dense1, training=self.training, name='bn1')\n",
    "        activate1 = nonlinear(bn1, name='activation1')\n",
    "\n",
    "        dense2    = tf.compat.v1.layers.dense(inputs=activate1, units=self.hidden_units, activation='linear', name='dense2')\n",
    "        bn2       = tf.compat.v1.layers.batch_normalization(dense2, training=self.training, name='bn2')\n",
    "        activate2 = nonlinear(bn2, name='activation2')\n",
    "\n",
    "        ## Output\n",
    "        O_ELASTICNET  = tf.expand_dims(tf.reduce_sum(input_tensor=tf.multiply(dense0, tf.one_hot(I_batch, depth=self._S_shape[1])), axis=1), axis=-1)\n",
    "        O_DNN = tf.compat.v1.layers.dense(inputs=activate2, units=1, activation='linear', name='output')\n",
    "\n",
    "        ## LOSS\n",
    "        LOSS_ELASTICNET  = tf.reduce_mean(input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(logits=O_ELASTICNET, labels=tf.cast(Y_batch, dtype=tf.float32)))\n",
    "        LOSS_DNN = tf.reduce_mean(input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(logits=O_DNN, labels=tf.cast(Y_batch, dtype=tf.float32)))\n",
    "        self.LOSS = 0.5 * (LOSS_DNN + LOSS_ELASTICNET)\n",
    "\n",
    "        ## OPTIMIZER 1st\n",
    "        OPT_ftrl = tf.compat.v1.train.FtrlOptimizer(learning_rate=self.learning_rate_ftrl,\n",
    "                                           l1_regularization_strength=self.l1_regularization_strength,\n",
    "                                           l2_regularization_strength=self.l2_regularization_strength)\n",
    "        var_list_ftrl = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'dense0')\n",
    "\n",
    "        ## OPTIMIZER 2nd\n",
    "        OPT_adam = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate_adam)\n",
    "        var_list_adam = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'dense1')\n",
    "        var_list_adam += tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'dense2')\n",
    "        var_list_adam += tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'output')\n",
    "        var_list_adam += tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'batchnormalization1')\n",
    "        var_list_adam += tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'batchnormalization2')\n",
    "\n",
    "        ## MINIMIZATION\n",
    "        update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.TRAIN_ftrl = OPT_ftrl.minimize(self.LOSS, var_list=var_list_ftrl)\n",
    "            self.TRAIN_adam = OPT_adam.minimize(self.LOSS, var_list=var_list_adam)\n",
    "\n",
    "        ## PREDICTION\n",
    "        self.P_main = tf.sigmoid(O_DNN)\n",
    "        self.Y_main = tf.cast(self.P_main > 0.5, tf.uint8)\n",
    "\n",
    "        ## ACCURACY\n",
    "        self.ACCURACY = tf.compat.v1.metrics.accuracy(labels=Y_batch, predictions=self.Y_main)\n",
    "        ## PRECISION\n",
    "        self.PRECISION = tf.compat.v1.metrics.precision(labels=Y_batch, predictions=self.Y_main)\n",
    "        ## RECALL\n",
    "        self.RECALL = tf.compat.v1.metrics.recall(labels=Y_batch, predictions=self.Y_main)\n",
    "\n",
    "        ## SAVE AND RESTORE\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "    def _open_session(self):\n",
    "        ## Create a new session\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "\n",
    "    def _close_session(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
